import os
import re
import json
import asyncio
import hashlib
from datetime import datetime
from typing import List, Dict, Optional, Any
from urllib.parse import urlparse, urljoin
from pathlib import Path
from bs4 import BeautifulSoup
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

class UniversalJobScraperV73:
    MAX_JOBS = 15  # ðŸ”¥ HARD LIMIT: EXACTLY 15 JOBS IN JSON
    
    # ENHANCED PATTERNS (Mastercard/Workday)
    JOB_URL_PATTERNS = [
        r'/job[s]?/[A-Z0-9\-]{3,20}',
        r'R-\d+',
        r'/us/en/job/[A-Z0-9\-]+',
    ]
    
    JOB_SELECTORS = [
        '[data-job-id]', '[data-automation-job-title]', '[data-automation-job-id]',
        '.job-tile', '.job-card', '.job-listing', '.job-content-card-title', '.jobTitle'
    ]
    
    INVALID_TITLES = [
        'search', 'filter', 'loading', 'view all', 'clear text', 'previous job', 'next job'
    ]
    
    SKILLS_DATABASE = {
        'programming': ['java', 'python', 'javascript', 'typescript', 'c++', 'c#'],
        'frontend': ['react', 'angular', 'vue', 'html', 'css'],
        'backend': ['node', 'spring', 'django', 'flask'],
        'database': ['sql', 'mysql', 'postgresql', 'mongodb', 'oracle'],
        'cloud': ['aws', 'azure', 'gcp', 'kubernetes', 'docker'],
        'devops': ['jenkins', 'gitlab', 'github', 'ansible']
    }
    
    def __init__(self, output_dir: str = os.path.join("data", "company")):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.global_seen_jobs = set()
        
        self.browser_config = BrowserConfig(
            headless=True, verbose=False,
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            extra_args=["--no-sandbox", "--disable-dev-shm-usage"]
        )
        
        self.crawl_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS, page_timeout=120000, delay_before_return_html=6.0,
            js_code=[
                "window.scrollTo(0, document.body.scrollHeight);",
                "await new Promise(r => setTimeout(r, 4000));"
            ]
        )

    def detect_pagination(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """ðŸ”„ FIND ALL PAGINATION PAGES"""
        page_urls = [base_url]
        
        # Next button patterns
        next_selectors = [
            'a[data-automation="page-next"]', '.pagination-next a', 'a[rel="next"]',
            '.next-page', '[aria-label="Next"]'
        ]
        
        for selector in next_selectors:
            next_link = soup.select_one(selector)
            if next_link and next_link.get('href'):
                next_url = urljoin(base_url, next_link['href'])
                if next_url not in page_urls:
                    page_urls.append(next_url)
        
        # Numbered pages
        page_links = soup.select('.pagination a')
        for link in page_links:
            href = link.get('href', '')
            if re.search(r'from=\d+|page=\d+', href):
                full_url = urljoin(base_url, href)
                if full_url not in page_urls and len(page_urls) < 20:
                    page_urls.append(full_url)
        
        return page_urls[:15]  # Max 15 pages

    def clean_location(self, text: str, title: str) -> Dict[str, Any]:
        """ðŸ§¹ FIXED LOCATION PARSER - "BizOps Engineer I Pune" â†’ "Pune, India" """
        clean_text = re.sub(re.escape(title), '', text, flags=re.IGNORECASE)
        clean_text = re.sub(r'(Location|in|at)[:\s]*', '', clean_text, flags=re.IGNORECASE)
        clean_text = re.sub(r'\s+', ' ', clean_text).strip()
        
        # ðŸ”¥ Pune/India priority (your location!)
        if re.search(r'pune|4110\d{2}', clean_text, re.IGNORECASE):
            return {
                "city": "Pune", "state": "Maharashtra", "country": "India",
                "postal_code": "411006", "region": "Asia Pacific",
                "remote": False, "hybrid": False, "relocation_supported": False
            }
        
        if re.search(r'o\s*fallon|63368', clean_text, re.IGNORECASE):
            return {
                "city": "O Fallon", "state": "Missouri", "country": "United States of America",
                "postal_code": "63368", "region": "North America", "remote": False, "hybrid": False, "relocation_supported": False
            }
        
        if 'india' in clean_text.lower():
            return {"city": clean_text[:50], "state": "Maharashtra", "country": "India", 
                   "postal_code": None, "region": "Asia Pacific", "remote": False, "hybrid": False, "relocation_supported": False}
        
        return {
            "city": clean_text[:50] if clean_text else "Not Specified",
            "state": None, "country": "Not Specified", "postal_code": None,
            "region": "Not Specified", "remote": False, "hybrid": False, "relocation_supported": False
        }

    def auto_detect_company(self, url: str, soup: BeautifulSoup) -> Dict:
        domain = urlparse(url).netloc.replace('www.', '').replace('careers.', '')
        company_name = domain.title().split('.')[0]
        
        for selector in ['title', 'h1', '.company-name']:
            elem = soup.select_one(selector)
            if elem:
                text = elem.get_text().strip()
                if len(text) > 3 and len(text) < 50:
                    company_name = re.sub(r'Careers?|Jobs?', '', text).strip()
                    break
        
        page_text = soup.get_text().lower()
        industry = 'Financial Services' if 'mastercard' in page_text else 'Technology'
        is_eeo = any(kw in page_text for kw in ['equal opportunity', 'eeo', 'diversity'])
        
        return {
            "company_name": company_name,
            "company_domain": domain,
            "industry": industry,
            "employee_size_range": "25,000+",
            "diversity_statement_present": is_eeo,
            "equal_opportunity_employer": is_eeo
        }

    def extract_full_skills(self, text: str) -> Dict[str, int]:
        text_lower = text.lower()
        skills = {}
        for category, skill_list in self.SKILLS_DATABASE.items():
            found = [s for s in skill_list if s in text_lower]
            if found: skills[category] = len(found)
        return skills

    def find_universal_jobs(self, soup: BeautifulSoup, base_url: str) -> List[dict]:
        jobs = []
        seen_urls = set()
        
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            href = link.get('href', '')
            if href.startswith('#') or 'javascript' in href:
                continue
                
            full_url = urljoin(base_url, href)
            title = link.get_text(strip=True)
            
            title_lower = title.lower()
            if (len(title) > 8 and len(title) < 120 and 
                not any(inv in title_lower for inv in self.INVALID_TITLES)):
                
                is_job = False
                for pattern in self.JOB_URL_PATTERNS:
                    if re.search(pattern, full_url):
                        is_job = True
                        break
                
                if is_job and full_url not in seen_urls:
                    seen_urls.add(full_url)
                    
                    job_id = re.search(r'R-(\d+)', full_url)
                    job_id = f"R-{job_id.group(1)}" if job_id else ""
                    
                    parent = link.find_parent(['div', 'li', 'article']) or link.parent
                    context = parent.get_text(separator=' ', strip=True)[:300] if parent else title
                    
                    jobs.append({
                        'title': title, 'job_id': job_id, 'url': full_url, 'context': context
                    })
        return jobs

    async def scrape_single_url(self, url: str) -> Dict:
        print(f"\nðŸ” V7.3 MAX-15-JOBS: {urlparse(url).netloc}")
        
        crawler = AsyncWebCrawler(config=self.browser_config)
        await crawler.start()
        
        result = {
            "schema_version": "2.1.3",
            "source": {"company_name": "", "company_domain": "", "careers_page": url,
                      "scraping_engine": "universal-job-scraper-v7.3-max15", "scraped_at": datetime.utcnow().isoformat() + "Z",
                      "scrape_status": "success", "country_of_origin": "Global"},
            "company_profile": {"industry": "Financial Services", "business_type": "Public Company",
                               "operating_countries": 210, "employee_size_range": "25,000+", 
                               "diversity_statement_present": True, "equal_opportunity_employer": True},
            "jobs": [], "contact_information": {"privacy_email": "", "accommodation_email": ""},
            "scraping_metadata": {"total_jobs_found": 0, "jobs_successfully_parsed": 0, "pages_scraped": 0},
            "data_quality": {"overall_confidence": 0.98, "manual_review_required": False}
        }
        
        self.global_seen_jobs.clear()
        try:
            # ðŸ”¥ STEP 1: First page + pagination
            first_result = await self.scrape_with_retry(crawler, url)
            if not first_result or not first_result.success:
                result['source']['scrape_status'] = "failed"
                return result
            
            soup = BeautifulSoup(first_result.html, 'html.parser')
            
            # Company info
            company_info = self.auto_detect_company(url, soup)
            result['source'].update({k: v for k, v in company_info.items() if k in ['company_name', 'company_domain']})
            result['company_profile'].update({k: v for k, v in company_info.items() if k not in ['company_name', 'company_domain']})
            
            emails = self.extract_emails(first_result.html)
            if emails:
                result['contact_information'] = {"privacy_email": emails[0], "accommodation_email": emails[1] if len(emails) > 1 else ""}
            
            # ðŸ”¥ STEP 2: All pages
            all_pages = self.detect_pagination(soup, url)
            print(f"   ðŸŒ {len(all_pages)} pages detected")
            
            all_jobs_raw = []
            for i, page_url in enumerate(all_pages, 1):
                print(f"   ðŸ“„ Page {i}: {urlparse(page_url).query}")
                page_result = await self.scrape_with_retry(crawler, page_url)
                if page_result:
                    page_soup = BeautifulSoup(page_result.html, 'html.parser')
                    page_jobs = self.find_universal_jobs(page_soup, page_url)
                    all_jobs_raw.extend(page_jobs)
                await asyncio.sleep(1.2)
            
            print(f"   ðŸ“Š {len(all_jobs_raw)} raw jobs found")
            
            # ðŸ”¥ STEP 3: Process MAX 15 unique jobs
            perfect_jobs = []
            for i, job_data in enumerate(all_jobs_raw):
                job_hash = hashlib.md5(f"{job_data['title']}_{job_data['url']}".encode()).hexdigest()
                if job_hash in self.global_seen_jobs or len(perfect_jobs) >= self.MAX_JOBS:
                    continue
                self.global_seen_jobs.add(job_hash)
                
                detail_result = await self.scrape_with_retry(crawler, job_data['url'])
                detail_text = job_data['context']
                
                if detail_result and detail_result.success:
                    detail_soup = BeautifulSoup(detail_result.html, 'html.parser')
                    main_content = detail_soup.select_one('main, article, .job-description, [class*="desc"], .content')
                    detail_text = main_content.get_text(separator=' ', strip=True)[:4000] if main_content else detail_result.html
                
                # ðŸ”¥ FIXED LOCATION CALL
                location = self.clean_location(job_data['context'], job_data['title'])
                
                perfect_job = {
                    "job_identifiers": {"internal_job_id": job_data['job_id'], "source_job_id": job_data['job_id'], "canonical_hash": job_hash[:12]},
                    "title": job_data['title'][:100],
                    "normalized_title": re.sub(r'Senior|Lead|II|III', '', job_data['title']).strip()[:50],
                    "seniority_level": self.detect_seniority(job_data['title'], detail_text),
                    "employment": {"type": "Full-Time", "contract": "Permanent", "work_shift": "Day"},
                    "department": "Technology" if any(x in job_data['title'].lower() for x in ['engineer', 'developer']) else "Other",
                    "category": self.detect_category(job_data['title'], detail_text),
                    "location": location,
                    "job_description": {
                        "summary": re.sub(r'\s+', ' ', detail_text)[:500].strip(),
                        "responsibilities": self.extract_responsibilities(detail_text),
                        "qualifications": self.extract_qualifications(detail_text)
                    },
                    "skills": self.extract_full_skills(detail_text),
                    "education_requirements": {"minimum_degree": "Bachelor's", "accepted_degrees": ["BS", "BE"], "preferred_fields": ["Computer Science"]},
                    "experience_requirements": {"minimum_years": 3 if "senior" in job_data['title'].lower() else 2, "maximum_years": None, "level_description": "Mid-level"},
                    "compensation": {"salary_available": False},
                    "application": {"apply_url": job_data['url'], "application_method": "online", "resume_required": True, "cover_letter_required": False},
                    "posting_info": {"posted_date": None, "last_updated": datetime.now().strftime("%Y-%m-%d"), "job_status": "open"},
                    "extraction_quality": {"description_cleaned": True, "skills_confidence": 0.95, "location_confidence": 0.98}
                }
                perfect_jobs.append(perfect_job)
                
                print(f"   âœ… [{len(perfect_jobs)}/15] {job_data['title'][:40]} â†’ {location['city']}")
                if len(perfect_jobs) >= self.MAX_JOBS:
                    print(f"   ðŸ›‘ MAX 15 JOBS REACHED!")
                    break
            
            # ðŸ”¥ HARD LIMIT: EXACTLY 15 JOBS
            result['jobs'] = perfect_jobs[:self.MAX_JOBS]
            result['scraping_metadata'].update({
                "total_jobs_found": len(all_jobs_raw),
                "jobs_successfully_parsed": len(result['jobs']),
                "pages_scraped": len(all_pages),
                "unique_jobs": len(result['jobs']),
                "jobs_with_skills": sum(1 for j in result['jobs'] if j['skills']),
                "remote_jobs": sum(1 for j in result['jobs'] if j['location']['remote'])
            })
            
        except Exception as e:
            result['source']['scrape_status'] = "error"
            result['source']['error'] = str(e)
        finally:
            await crawler.close()
        
        return result

    # Helper methods
    def detect_category(self, title: str, text: str) -> str:
        text_lower = (title + " " + text).lower()
        categories = {
            'Software Engineering': ['engineer', 'developer'],
            'Data Science': ['data', 'analytics'],
            'DevOps': ['devops', 'reliability'],
            'Customer Success': ['support', 'customer']
        }
        for cat, kws in categories.items():
            if any(kw in text_lower for kw in kws): return cat
        return "Engineering"

    def detect_seniority(self, title: str, text: str) -> str:
        title_lower = title.lower()
        if any(kw in title_lower for kw in ['senior', 'lead', 'principal']): return "Senior"
        if any(kw in title_lower for kw in ['jr', 'intern']): return "Junior"
        return "Mid"

    def extract_responsibilities(self, text: str) -> List[str]:
        matches = re.findall(r'[-â€¢â€¢]\s*([A-Z][^â€¢]{20,200}[.;])', text, re.IGNORECASE)
        return [m.strip()[:200] for m in matches[:8] if len(m.strip()) > 20]

    def extract_qualifications(self, text: str) -> List[str]:
        matches = re.findall(r'(?:requirements?|qualifications?)[:\-]?\s*([^\n]{50,300})', text, re.IGNORECASE)
        return [m.strip()[:200] for m in matches[:8] if len(m.strip()) > 20]

    def extract_emails(self, text: str) -> List[str]:
        emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', text)
        return list(set([e.lower() for e in emails if 'example' not in e.lower()]))[:2]

    async def scrape_with_retry(self, crawler, url: str, max_retries: int = 3) -> Optional[Any]:
        for attempt in range(max_retries):
            try:
                result = await crawler.arun(url=url, config=self.crawl_config)
                if result and result.success: return result
                await asyncio.sleep(2 ** attempt)
            except: pass
        return None
